{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YouTube search/archival notebook ",
      "provenance": [],
      "collapsed_sections": [
        "xzqbHdNR6m1i"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thecatbaron/testing/blob/master/YouTube_search_archival_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grBRevcO5W1W"
      },
      "source": [
        "# **YouTube search/archival notebook** \n",
        "Notebook for querying Youtube for topic/tags, video metadata/engagement archiving, comments, and video/thumb download. Retrieves data from the YouTube API. Requires Python 3.\n",
        "\n",
        "*Currently this is a work in progress!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iEsDkQ26k6F"
      },
      "source": [
        "## **Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzqbHdNR6m1i"
      },
      "source": [
        "### Imports and defines\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6rKzmDY9mHJ",
        "outputId": "d07013a2-3a53-447d-f81b-39c79060af7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install youtube-dl\n",
        "!apt-get -qq install -y atomicparsley ffmpeg\n",
        "!pip install dprint\n",
        "!pip install pytchat\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import requests\n",
        "import datetime\n",
        "import os\n",
        "import youtube_dl\n",
        "import pytchat\n",
        "from requests.exceptions import ConnectionError\n",
        "from requests.packages.urllib3.exceptions import ProtocolError\n",
        "from urllib.parse import parse_qs\n",
        "from ipywidgets import IntProgress\n",
        "from dprint import dprint\n",
        "from __future__ import unicode_literals"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting youtube-dl\n",
            "  Downloading youtube_dl-2021.12.17-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 3.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: youtube-dl\n",
            "Successfully installed youtube-dl-2021.12.17\n",
            "Selecting previously unselected package atomicparsley.\n",
            "(Reading database ... 155639 files and directories currently installed.)\n",
            "Preparing to unpack .../atomicparsley_0.9.6-1_amd64.deb ...\n",
            "Unpacking atomicparsley (0.9.6-1) ...\n",
            "Setting up atomicparsley (0.9.6-1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dprint\n",
            "  Downloading dprint-0.1.0-py2.py3-none-any.whl (7.4 kB)\n",
            "Installing collected packages: dprint\n",
            "Successfully installed dprint-0.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytchat\n",
            "  Downloading pytchat-0.5.5-py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting httpx[http2]\n",
            "  Downloading httpx-0.23.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx[http2]->pytchat) (2022.6.15)\n",
            "Collecting httpcore<0.16.0,>=0.15.0\n",
            "  Downloading httpcore-0.15.0-py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 6.7 MB/s \n",
            "\u001b[?25hCollecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting h2<5,>=3\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting hpack<5,>=4.0\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Collecting hyperframe<7,>=6.0\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting anyio==3.*\n",
            "  Downloading anyio-3.6.1-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 9.2 MB/s \n",
            "\u001b[?25hCollecting h11<0.13,>=0.11\n",
            "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from anyio==3.*->httpcore<0.16.0,>=0.15.0->httpx[http2]->pytchat) (2.10)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from anyio==3.*->httpcore<0.16.0,>=0.15.0->httpx[http2]->pytchat) (4.1.1)\n",
            "Installing collected packages: sniffio, rfc3986, h11, anyio, hyperframe, httpcore, hpack, httpx, h2, pytchat\n",
            "Successfully installed anyio-3.6.1 h11-0.12.0 h2-4.1.0 hpack-4.0.0 httpcore-0.15.0 httpx-0.23.0 hyperframe-6.0.1 pytchat-0.5.5 rfc3986-1.5.0 sniffio-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GX__8gmLFes"
      },
      "source": [
        "def search(query_keyword, output_name, max_results=25, geocode=None, event_type=None, order=\"date\", lang=None):\n",
        "  \"\"\"\n",
        "    Pass in a keyword and output name (e.g. \"kamala_biden\") with optional max_results (max 50), \n",
        "    geocode (e.g. 38.7442,-90.3054,1mi), event_type (\"live\", \"completed\", \"upcoming\"), \n",
        "    order (\"date\", \"title\" (alphabetical), \"viewCount\" (popularity), \"rating\", \"relevance\"),\n",
        "    or lang (iso639-2). Writes output to jsonl file.\n",
        "    Ordering defaults to recent (i.e. not mixed, the API default, or popular) YouTube videos.\n",
        "  \"\"\"\n",
        "  url = \"https://www.youtube.com/watch?v=tL3twECKpQ0\"\n",
        "  params = {\n",
        "            \"part\": \"snippet\",\n",
        "            \"q\": query_keyword,\n",
        "            \"maxResults\": max_results,\n",
        "            \"type\": \"video\",\n",
        "            \"order\": order,\n",
        "            \"key\": API_key\n",
        "        } \n",
        "  if lang is not None:\n",
        "    params['lang'] = lang\n",
        "  if geocode is not None:\n",
        "    params['geocode'] = geocode\n",
        "  resp = requests.get(url, params=params) \n",
        "\n",
        "  output_name_jsonl = output_name + '.jsonl'\n",
        "  \n",
        "  with open(output_name_jsonl, 'w') as outfile:\n",
        "    for video in resp.json()[\"items\"]:\n",
        "        # double encoding json = no good\n",
        "        # json.dump(c.json(), outfile)\n",
        "        # outfile.write('\\n')\n",
        "        outfile.write(c.json() + '\\n')\n",
        "\n",
        "\n",
        "\n",
        "def get_df(youtube_jsonl): \n",
        "  \"\"\"\n",
        "    Pass in youtube_jsonl (e.g. 'output.jsonl') and get a dataframe of 8 columns: \n",
        "    'id.kind': string, The type of the API resource (e.g. \"video\", \"channel\")\n",
        "    'id.videoId': string, The unique video ID\n",
        "    'snippet.publishedAt': datetime, The creation date and time of the resource\n",
        "    'snippet.channelId': string, The unique channel ID\n",
        "    'snippet.title': string, The title of the search result\n",
        "    'snippet.description': string, A description of the search result.\n",
        "    'snippet.thumbnails.default.url': string, URL of the default thumbnail image\n",
        "    'snippet.channelTitle': string, The title of the channel that published the resource that the search result identifies\n",
        "  \"\"\"\n",
        "  df = pd.json_normalize(pd.Series(open(youtube_jsonl).readlines()).apply(json.loads))\n",
        "  # only keep interesting columnsb\n",
        "  df = df[['id.kind','id.videoId','snippet.publishedAt', 'snippet.channelId', 'snippet.title', 'snippet.description', 'snippet.thumbnails.default.url', 'snippet.channelTitle']]\n",
        "  # clean data\n",
        "  df['id.kind'] = df['id.kind'].str.replace(r'youtube#', '')\n",
        "  df['snippet.publishedAt'] = df['snippet.publishedAt'].astype('datetime64[ns]')\n",
        "  return df \n",
        "\n",
        "\n",
        "\n",
        "def is_yt_video_live(video_link_or_id):\n",
        "  \"\"\" \n",
        "    Pass in Youtube video url or video ID string and get True if video is live, False if not\n",
        "  \"\"\"\n",
        "  with YoutubeDL({'ignoreerrors': True, \"quiet\": True}) as ydl: # for some reason, adding \"is_live\": True to options is not effective\n",
        "    info_dict = ydl.extract_info(video_link_or_id, download=False)\n",
        "    # info_dict.get('is_live', None) returns None if not live, True if live\n",
        "    return True if info_dict.get('is_live', None) else False\n",
        "    \n",
        "\n",
        "def download_video(list_of_video_links, include_thumbnail_download=True): \n",
        "  \"\"\" \n",
        "    Pass in a list of Youtube URLs and \n",
        "    include_thumbnail_download bool (default True) to download \n",
        "    thumbnails along with videos, or to download videos only. \n",
        "  \"\"\"\n",
        "  ydl_opts = {\n",
        "    'writethumbnail': include_thumbnail_download,\n",
        "    'postprocessors': [\n",
        "        # remove commented lines to download audio only\n",
        "        #{\n",
        "        #    'key': 'FFmpegExtractAudio',\n",
        "        #    'preferredcodec': 'mp3',\n",
        "        #}, \n",
        "        {'key': 'EmbedThumbnail'},\n",
        "        {'key': 'FFmpegMetadata'},\n",
        "    ],\n",
        "    'retries':4, \n",
        "    'ignoreerrors': True, \n",
        "    'format': 'mp4', \n",
        "    'subtitleslangs': ['en'], \n",
        "    'writeautomaticsub': True, \n",
        "    'convertsubtitles': 'srt', \n",
        "    'restrictfilenames': True\n",
        "  }   \n",
        "  try: \n",
        "    with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
        "      ydl.download(list_of_video_links)\n",
        "  except youtube_dl.utils.DownloadError as e:\n",
        "    dprint(e)\n",
        "  return\n",
        "\n",
        "\n",
        "def download(link): \n",
        "  \"\"\" \n",
        "    Python equivalent of a given wget command, \n",
        "    best applied to download thumbnail urls\n",
        "  \"\"\"\n",
        "  os.system('wget %s'%link)\n",
        "  return\n",
        "\n",
        "def get_more_video_info(video_id): \n",
        "  \"\"\" \n",
        "    Given a video ID, returns a dictionary that contains: \n",
        "    - number of likes it's received\n",
        "    - number of comments\n",
        "    - number of favorites\n",
        "    - number of dislikes \n",
        "    - view count\n",
        "    - cateogory ID \n",
        "    - tags/hashtags\n",
        "  \"\"\"\n",
        "  url = \"https://www.googleapis.com/youtube/v3/videos?part=snippet&part=statistics\"\n",
        "  params = {\n",
        "            \"id\": video_id,\n",
        "            \"key\": API_key\n",
        "        } \n",
        "  r = requests.get(url, params=params) \n",
        "  video_dict = r.json()['items'][0]['statistics']\n",
        "  video_dict['cateogoryId'] = r.json()['items'][0]['snippet']['categoryId']\n",
        "  video_dict['tags'] = r.json()['items'][0]['snippet']['tags']\n",
        "  return video_dict\n",
        "\n",
        "\n",
        "def merge_df_with_more_video_info(df): \n",
        "  \"\"\" \n",
        "    Concatenates df with the dictionary\n",
        "    that is formed by get_more_video_info \n",
        "    and returns the final df\n",
        "  \"\"\"\n",
        "  more_df = [] \n",
        "  for x in df['id.videoId']: \n",
        "    more_df.append(get_more_video_info(x))\n",
        "  df2 = pd.DataFrame(more_df)\n",
        "  return pd.concat([df, df2], axis=1)\n",
        "\n",
        "\n",
        "def get_yt_livestream_comments(video_url_or_id, output_name=\"output\"):\n",
        "  \"\"\" \n",
        "     Given a livestream video ID or URL to livestream video (where the livestream chat is \n",
        "     currently active, or has ended BUT the Live Chat Replay is enabled), \n",
        "     writes .jsonl file of livestream comments. \n",
        "     Example use: \n",
        "     >>> url = \"https://www.youtube.com/watch?v=fJKBM6WGR7s\"\n",
        "     >>> output_name = \"youtube_video_election\" # your file will be saved as \"youtube_video_election.jsonl\"\n",
        "     >>> get_yt_livestream_comments(url, output_name)\n",
        "  \"\"\"\n",
        "  match = re.search('((?<=(v|V)/)|(?<=be/)|(?<=(\\?|\\&)v=)|(?<=embed/))([\\w-]+)', video_url_or_id)\n",
        "  if match: # Extract video ID from YouTube link\n",
        "    video_url_or_id = match.group(0)\n",
        "  output_name_jsonl = output_name + '.jsonl'\n",
        "  chat = pytchat.create(video_url_or_id)\n",
        "  with open(output_name_jsonl, 'w') as outfile:\n",
        "    while chat.is_alive():\n",
        "      for c in chat.get().items:\n",
        "        outfile.write(c.json() + '\\n')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6eIj-lg6qVy"
      },
      "source": [
        "### YouTube keys"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soXXtzfN-CKK"
      },
      "source": [
        "API_key = \"AIzaSyAJCJnRynX12B1qgibW6kyVNRQbP8z8LVY\" # insert your API key here"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhJNcIyrn4al"
      },
      "source": [
        "# Video download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68UPsuFBtbhG",
        "outputId": "d1f4ffba-e5a4-4777-f2ee-c43466945eed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "url = \"https://www.youtube.com/watch?v=y8Kyi0WNg40\" # insert your link here\n",
        "download_video([url]) # notice that it takes in a list of strings, so you can also create an array of youtube links and pass it in"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] y8Kyi0WNg40: Downloading webpage\n",
            "[youtube] y8Kyi0WNg40: Downloading player 0e7373c2\n",
            "[youtube] y8Kyi0WNg40: Downloading thumbnail ...\n",
            "[youtube] y8Kyi0WNg40: Writing thumbnail to: Dramatic_Look-y8Kyi0WNg40.jpg\n",
            "[download] Destination: Dramatic_Look-y8Kyi0WNg40.mp4\n",
            "[download] 100% of 194.28KiB in 00:03\n",
            "[ffmpeg] Correcting extension to webp and escaping path for thumbnail \"Dramatic_Look-y8Kyi0WNg40.jpg\"\n",
            "[ffmpeg] Converting thumbnail \"Dramatic_Look-y8Kyi0WNg40.webp\" to JPEG\n",
            "[atomicparsley] Adding thumbnail to \"Dramatic_Look-y8Kyi0WNg40.mp4\"\n",
            "[ffmpeg] Adding metadata to 'Dramatic_Look-y8Kyi0WNg40.mp4'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIB75uKBnhDR"
      },
      "source": [
        "# YouTube Live Comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsnTsWf1fqSZ"
      },
      "source": [
        "url = \"https://www.youtube.com/watch?v=fJKBM6WGR7s\"\n",
        "output_name = \"youtube_video_election\" # your file will be saved as \"youtube_video_election.jsonl\"\n",
        "get_yt_livestream_comments(url, output_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upC5c5l2wRkN"
      },
      "source": [
        "# Example Uses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqzh22NZwU7d"
      },
      "source": [
        "Let's say you wanted to get information of videos that matched the keyword \"election\" and was \"live\" on YouTube."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylsV4q-KDlMf"
      },
      "source": [
        "search(\"election\", \"output.jsonl\", event_type=\"live\") # this writes a json of the results \n",
        "# to view your json inline as a df\n",
        "yt_results = get_df(\"output.jsonl\")\n",
        "yt_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdqiolMxiS2-"
      },
      "source": [
        "Great, now you want to download the videos along with its thumbnail. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwH_-cxkiScf"
      },
      "source": [
        "# to form the video url you have to prefix the video id with the youtube domain e.g. \"https://www.youtube.com/watch?v=\" + yt_results[\"id.videoId\"].astype(str)\n",
        "video_links = download_video(\"https://www.youtube.com/watch?v=\" + yt_results[\"id.videoId\"].astype(str))\n",
        "download_video(video_links) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0rMiFJSl1pN"
      },
      "source": [
        "If you only wanted to download 1 video, without the thumbnail, it would look like: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8HU6Iqvl4s-"
      },
      "source": [
        "download_video([video_links[0]], False) # for the second video, you do NOT want the thumbnail to be downloaded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stk0NZZ6mxez"
      },
      "source": [
        "# **Storage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yD71AuBmgpa"
      },
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "# The path to copy our data to\n",
        "OUT_PATH = '/content/drive/Shared drives/Election Integrity Partnership/Raw Data/YouTube/'\n",
        "\n",
        "# Set this to the current ticket\n",
        "ticket = \"EIP-xxx\"\n",
        "\n",
        "!mkdir -p \"{OUT_PATH}/{ticket}\"\n",
        "!cp *.jsonl \"{OUT_PATH}/{ticket}/\"\n",
        "!cp *.jpg \"{OUT_PATH}/{ticket}/\" 2>/dev/null\n",
        "!cp *.mp3 \"{OUT_PATH}/{ticket}/\" 2>/dev/null\n",
        "!cp *.mp4 \"{OUT_PATH}/{ticket}/\" 2>/dev/null\n",
        "!cp *.mkv \"{OUT_PATH}/{ticket}/\" 2>/dev/null"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}